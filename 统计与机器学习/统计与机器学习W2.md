# 统计与机器学习W2

## Pre

1. $\sum \alpha_i=0$,唯一吗？能替换吗？加约束的目的是使参数估计时具有识别性。

   这a+1个方程并不是都是线性无关的，a 个方程无法估计a+1个参数，所以添加约束使参数可识别了。

   逻辑上，添加与a个方程线性无关即可估计？

   Why $\sum \alpha_i=0$?

2. 从数学上无法解释=》逻辑上解释

   $\sum \alpha_i=0$说明了这个Overall的$\mu$，$\alpha_i$表示因子A处于第i个水平的效应。

   $\sum \alpha_i=0$说明效应和是0，or效应均值是0.

   $a_1=0$，把第一组的均值$\mu_1$当作baseline， 即把某一个水平看做一种参照系，验证其他因子和他之间是否存在差异。

3. $\hat{\sigma^2}=\frac{1}{n-a}SS_e$很重要，是代入计算。无论是原还是H1都成立

4. $\hat{\mu_i}$是什么？

   $\hat{\mu}=\bar{y_{..}},\hat{\alpha_i}=\bar{y_{i.}}-\bar{y_{..}}$

   $\hat{\mu_i}=\hat{\mu}+\hat{\alpha_i}$，由MLE的不变形保证。

   $\hat{\mu_i}=\bar{y_{i.}}$

---

## 点估计-》置信区间

1. $\hat{\mu_i}$的点估计：$\hat{\mu_i}=\bar{y_{i.}}$

2. 点估计的分布:$y_{ij}$~$N(\mu+\alpha_i,\sigma^2)$,i固定时分布是确定的。

   所以$\bar{y_{i.}}=\frac{1}{m}\sum^m_{j=1} y_{ij}$~$N(\mu+\alpha_i,\sigma^2/m)$

3. 标准化$\hat{\mu_i}$～N(0,1)，

   G=标准化$\hat{\mu_i}$，G中会有讨厌因子$\sigma^2$,用讨厌因子的估计取代。

   这里使用$\sigma^2$的估计$\frac{1}{n-a}SS_e$。不用$SS_a$是因为$SS_a$只在$H_0$成立。

   带入后，即可得到$t(n-a)$，$t(n-a)$的构造独立性仍由每个组的均值与方差独立保证。
   $$
   t(n-a)=\frac{\bar{y_{i·}-\mu_i}}{\sqrt{\hat{\sigma^2}/m}}
   $$
   

4. 在置信水平为$1-\alpha$,$p(\theta_1 \le t \le \theta_2)=1-\alpha$,即可。

   所以$\mu_i$的置信区间为

   $\bar{y_{i.}}+-t_{1-\alpha/2}(n-a)\sqrt{\hat{\sigma^2}/m}$

## 模型诊断：判断数据是否符合模型

$y_{ij}=\mu+\alpha_i+,\epsilon_{ij}为N(0,1),i.i.d$

$y_{ij}$～$N(\mu+\alpha_i,\sigma^2)$

1. 观测具有独立性：不是很容易检验（难做）；在**数据收集阶段**介入可能会有独立性检验。

   DW检验试验次序对试验结果的影响，即时序上是相关的还是独立的。

2. 方差齐性，组和组的方差一致。

   Bartlett's Test，常用，对正态性很敏感。

   一般是先跑Bartlett's Test，不满足再Levene Test。

   Levene Test，非参，更稳健

3. 正态性假设：it is robust to the normality assumption. Q图尾部稍微偏移正态。

上面三个条件又强到弱。

正态性的偏移：偏移和厚尾.正态性中怕尾概率难以控制。

> 善于画图

## 数据不是齐性：对数据做变换--$Box-Cox$变换

$Box-Cox$变换前身是幂变换，$Box-Cox$变换相当于幂变换+对数变换。

### 经验法则：

1. 计数数据 Counting Data:先做对数变换。

2. 正数log-normal数据:对数 or 开方$y^*=\sqrt{1+y}$

   log-normal:$x~N(0,1);Y=log(x),Y 为logN(\mu,\sigma^2)$

3. 比率数据 y,区域某个人群占的比率，这个比率是介于(0,1)之间的小数。

   $y^*=arcsin(y)$

### 特殊情况：

如果方差与均值水平有关系，此时方差齐性就不满足了。

股票数据：收益均值越大，方差越大。期望越大，方差越小？

$\mu=E(y)$表示原始值的期望

$\sigma_y$正比于$\mu^{\alpha}$，表示原始值的标准差

幂变换$y^*=y^{1-\alpha}$

要证变换后的方差和期望无关。

$Var(y^*)=Var(y^{\lambda})=E(y^{\lambda})^2-(E(y^{\lambda}))^2$

$f(y)=y^{\lambda}$从数的角度去看，使用泰勒展开，$f(x)=f(x_0)+\frac{1}{1!}f'(x_0)(x-x_0)+\frac{1}{2!}f^{(2)}(x_0)(x-x_0)^2+O((x-x_0)^2)$

这里看成$y$的$\lambda$次方展开，应用泰勒展开公式。

碰到复杂变量，在统计上技巧是应用其线性组合来表示。

$\mu^{2\lambda+2\alpha-2}=\mu^0$

推得$\lambda=1-\alpha$

在实际数据中如何求$\alpha$?

$\sigma_y$正比于$\mu^{\alpha}$，做log后,$\alpha$从指数上拉下来。$ln(\sigma_y)$=lnc+$\alpha ln(\mu)$

通过描点求斜率得到alpha.

### Box-Cox变换

1. $y(\lambda)=\begin{cases}\frac{y^{\lambda}-1}{\lambda}&&\lambda \neq0\\lny&&\lambda=0\end{cases}$
2. $y(\lambda)=\begin{cases}\frac{(y+\lambda_2)^{\lambda_1}-1}{\lambda_1}&&\lambda \neq0\\ln(y+\lambda_2)&&\lambda=0\end{cases}$

通常为了增加可解释性，一般会用更好算的数。如$\lambda=0.4$,开平方根变化，即$\lambda$近似于0.5，不过如果不追求可解释性也可以直接带进去算。



# Two-way Anova

## 一个例子

化肥 种类+使用量对亩产量的影响。即因素从化肥 种类的单一因素扩展到化肥使用量。

化肥 种类a种，使用量b种，重复m次。判断水稻平均亩产量。a$\times$b$\times$m

|      | 1       | 2       | 3      | 4      | b       |
| ---- | ------- | ------- | ------ | ------ | ------- |
| 1    | m个数据 | m个数据 | 。。。 | 。。。 | m个数据 |
| 2    | m个数据 | 。。。  | 。。。 | 。。。 |         |
| 3    |         | 。。。  | 。。。 | 。。。 |         |
| 4    |         | 。。。  | 。。。 | 。。。 |         |
| a    |         | 。。。  | 。。。 | 。。。 |         |

$y_{ijk},i=1,2,...,a;b=1,2,...,b;k=1,2,3,...,m;n=a,b,m$

## 模型，假设，方差分析表

