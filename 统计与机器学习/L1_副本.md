# kaggle-L1

防止garbage in garbage out!

## Procedure

- **Understand the Data**(大多数时间) 在理解数据基础上
- Data Cleaning
- Feature Engineering
- Modeling

数据的理解远比模型重要！！！

## 判断我们要处理什么问题

是Regression or Classification？

## Why Data Cleaning is Important?

![截屏2021-04-05 11.31.03](https://tva1.sinaimg.cn/large/008eGmZEgy1gp8pllllr3j311c0mk7g4.jpg)

红色是我们线性回归拟合出来的，而黑色是真实的，为什么红色会这么向下呢？

可以从散点图看出，右面下面的两个点，算outlier，它们极大的影响了我们拟合出来的东西的斜率。

##  Know the Data

### 偏度和封度

https://zhuanlan.zhihu.com/p/84614017

作业1:

偏度：左偏>0 ：偏度（Skewness）可以用来度量随机变量概率分布的不对称性。

封度: >3 有fat tail ：峰度的取值范围为[1,+∞)，完全服从正态分布的数据的峰度值为 3，峰度值越大，概率分布图越高尖，峰度值越小，越矮胖。

Medium 比 mean 要小

Tail:豪宅 

比较mean 和 medium

面积越大，房子越贵；

越新的房子越贵：做好统计项目：常识(common sense)是非常重要的，对数据有自己的理解，知道是什么东西很重要，防止garbage in garbage out.

数字与common sense违背时告诉我们什么

![截屏2021-04-09 21.28.42](https://tva1.sinaimg.cn/large/008eGmZEgy1gpdtcmik4gj30sy0hg3zx.jpg)

注意图里outlier对我们预测的不准。

美国的房子不是越新越贵 ，pre-war的房子比1950s的房子要贵。因为1920s的房子钢筋水泥结构好。曼哈顿pre-war房子。。

Numerical v.s. categorical features

![截屏2021-04-09 21.31.35](https://tva1.sinaimg.cn/large/008eGmZEgy1gpdtflsdx9j30sy0hy0ty.jpg)

分数越高房子越贵

## Explore the Data

![截屏2021-04-09 21.33.49](https://tva1.sinaimg.cn/large/008eGmZEgy1gpdthwvcx7j30sy0hy14h.jpg)

## 清理数据

### missingData

基于数据的理解

1. 直接删掉：直接不要会删除很多信息

缺失值补充保持 information保证不会影响我最后的建模

1.中位数补充：对于数字型来说 不一定一定是中位数是很少的 有时候是不合理的

2.字符型：

Mode 众数

作业2:缺失值填补

从原始数据可以看出，共有19个column带有缺失值

![截屏2021-04-10 08.03.38](/Users/chixinning/Desktop/截屏2021-04-10 08.03.38.png)

DataDescription中有NA的，比如PoolQc 就是没有的意思

Gatagorical:

PoolQc/Alley/Fence/FireplaceQu/GarageCond/GarageQual/BsmtExposure/BsmtFinType1/BsmtFinType2/BsmtQual/BsmtCond(主观臆断为没有就是没有，而且可以把此类categorical 转换成numerical)

/MiscFeature/GarageType

GarageYrBlt:没有就是没有，(Finished/RFn/Unf)

MasVnrType（这一类型缺失只能删）

Electrical: Electrical system:是否可转换成数值型存疑

Numerical:

LotFront：缺失值填补，数字型

MasVnrArea

MiscFeature: Miscellaneous feature not covered in other categories



outlier的影响

normal distribution 1std /2td/3td分别是多少的percentile

### 正态分布

作业3



![img](https://tva1.sinaimg.cn/large/008eGmZEgy1gpdu08x3wqj30ny08774w.jpg)

![img](https://tva1.sinaimg.cn/large/008eGmZEgy1gpdu03rg8qj30o408cdge.jpg)

![img](https://tva1.sinaimg.cn/large/008eGmZEgy1gpdu0b5uilj30o008c0td.jpg)

### outlier直接删掉

这次数据特征很像金融数据：

数据点少，但每一个点的features是相对比较多的，

股票日数据：2500个点(10年数据)，但同一个股票会有大量的features 

所以每一个点都是挺重要的，所以不太轻易删一个房子这种

c.f. 动辄上百万的互联网大数据

### ### Outlier  :直接对outlier 做log transformation？![截屏2021-04-09 21.57.33](https://tva1.sinaimg.cn/large/008eGmZEgy1gpdu6nb8e2j30sy044wfz.jpg)

The log transformation can be used to make highly [skewed](javascript:glossary('skew')) distributions less skewed. This can be valuable both for making patterns in the data more interpretable and for helping to meet the assumptions of inferential statistics.

### QQ图

只是把正态分布的拟合拉直了而已

对Fat-tail做对数处理后，把Fat-tail处理掉(？为什么呢)

### 如何把numerical 数据换成categorical数据?

![截屏2021-04-09 22.36.51](https://tva1.sinaimg.cn/large/008eGmZEgy1gpdvbifgkej30sy0b6jyr.jpg)

不存在强弱关系这种情况下就不能被当作数字型的features

但有强弱之分的字符型的features是可以被当数字型

作业4:找出所有需要从数字变成字符的变量；找出所有需要从字符变成数字的变量

统计里有很多的主观判断



---

存疑：为什么？

1. train_data["Electrical"]=train_data["Electrical"].fillna(train_data["Electrical"].mode()[0])要fill with mode?

2. 就那个16个图的可视化

![截屏2021-04-10 08.32.04](https://tva1.sinaimg.cn/large/008eGmZEgy1gpecits3gij30s604qt9o.jpg)

3. 缺失值处理方式

![截屏2021-04-10 08.32.24](https://tva1.sinaimg.cn/large/008eGmZEgy1gpecj6uyf3j30s60ritid.jpg)

![截屏2021-04-10 08.32.42](https://tva1.sinaimg.cn/large/008eGmZEgy1gpecjgks7tj30s608agng.jpg)