# 统计与机器学习W8:多重共线性(2)

1. 不要completely data driven

## 岭回归

### 多重共线性

变差：

1. 正向系数变负
2. 显著的东西不显著

导致变差的本质原因：

$|X^TX|\approx0$,求逆矩阵的时候典型方法是使用行列式和伴随矩阵，所以导致逆矩阵很大，不好球。导致beta的显著性很难检验。

在二元，相关性up，beta_1的方差和beta_2的方差都提高了。

### 既然求逆不好求

- OLS：$\hat{\beta}=(X^TX)^{-1}X^Ty$
- Ridge Estimator:$\hat{\beta(k)}=(X^TX+kI)^{-1}X^Ty$,这里的k可以类比于超参数，c.f.主成分分析里面的超参数。
- ![截屏2021-04-22 08.13.06](https://tva1.sinaimg.cn/large/008i3skNgy1gps7etn3s6j30sq0j8god.jpg)

注意这里的标准化，与我们之前定义的标准化本质上没啥子太大区别，也就是差一个常数【存疑】

- 岭回归估计族：the family of the estimations

  $B={\hat{\beta}{(k)},k\ge0}$

  这个定义的岭回归估计族里面包含了最小二乘估计$\hat{\beta}(0)$

  可以把最小二乘估计看成k取0的估计。一类特殊的岭估计

## 岭回归的性质

### 岭估计是有偏的

![截屏2021-04-22 08.19.55](https://tva1.sinaimg.cn/large/008i3skNgy1gps7luz22rj30pa0c475u.jpg)

### 岭回归估计与OLS估计的关系

$\hat{\beta(k)}=(X^TX+kI)^{-1}X^Ty$

$=(X^TX+kI)^{-1}(X^TX)(X^TX)^{-1}X^Ty$

把$(X^TX)^{-1}X^Ty$看成一体的，即为最小二乘估计

所以$\hat{\beta(k)}=(X^TX+kI)^{-1}(X^TX)\hat{\beta_{OLS}}$

- 如果k与y无关，则岭回归估计是OLSE的一种线性变换，岭回归估计也是y的线性函数。
- 但是k本质上(实际)上通过数据驱动选k的时候是依赖于y的，在这种意义上，岭回归不是OLSE的线性变换，也不是y的线性函数。即差异是存在的，但这种差异我们一般不聊，即把k当作超参数。

### 为什么岭回归比OLS更优？

一般用MSE标准

> 存在k>0，使MSE(岭估计)$\le$MSE(OLSE)

存在某一k，使$MSE(\hat{\beta}(k))\le MSE(\hat{\beta}(0))$

- 证明思路Outine:令$H(k)=MSE(\hat{\beta}(k))$,求$H(k)$在0处的导数，若$H'(k)<0,$那么我们的k就相当于找到了嘛！
- 如何求$H'(k)$？

![截屏2021-04-22 08.32.55](https://tva1.sinaimg.cn/large/008i3skNgy1gps7zevhlnj30pc094q3y.jpg)

注意高维不是被拆成了方差协方差矩阵。

因为不是无偏的，通常干的就是减期望加期望这种事。

### 岭估计的不同表达形式

![截屏2021-04-22 08.41.26](https://tva1.sinaimg.cn/large/008i3skNgy1gps888rrhoj30pc0hugo1.jpg)

$W_k$，$W_k^*$都是对称矩阵。

#### $W_k$，$W_k^*$之间的关系是什么？有两种

$W_k^*=W_k(X^TX). (1)$

$W_k^*=W_k(X^TX)=W_k(X^TX+kI-kI)=I-kW_k.  (2)$【一定要凑一个Wk的形式】

**$X^TX$的特征值记为$\lambda_1,\lambda_2,...,\lambda_p$，对应的特征向量是$v_1,v_2,...,v_p$，这里约定其为相互正交的单位特征向量。**实对称矩阵一定可以求特征值。

$(X^TX)v_i=\lambda_iv_i$

$(X^TX)=V^T\Lambda V,V^T=(v_1,v_2,..,v_p),\Lambda=diag(\lambda_1,\lambda_2,...,\lambda_p)$

#### $W_k$的特征值和特征向量是什么？

$(X^TX)v_i=\lambda_iv_i$；$kIv_i=kv_i;$$(X^TX+kI)v_i=(\lambda_i+k)v_i$

$(X^TX+kI)$的逆矩阵一定存在【存疑】

$(X^TX+kI)^{-1}v_i=(\lambda_i+k)^{-1}v_i$,这就是$W_k$的特征值和特征向量，想想矩阵要是乘逆矩阵，而不是要想单纯的除这种!

$W_k=V^T\Lambda_kV$

$(I-kW_k)v_i=v_i-kW_kv_i=(1-\frac{k}{\lambda_i+k})v_i$

所以，结合（2）,所以即可得$W_k^*v_i=\frac{\lambda_i}{\lambda_i+k}v_i$,就求出来它的特征值和特征向量了。

### 特征值和特征向量的总结

![截屏2021-04-22 08.59.20](https://tva1.sinaimg.cn/large/008i3skNgy1gps8qvqtyjj30se09oabt.jpg)

K只影响在特征值而不影响特征向量。

### $I_1(k)$

![截屏2021-04-22 09.04.14](https://tva1.sinaimg.cn/large/008i3skNgy1gps8vznxmvj30t80iaq58.jpg)

虽然很复杂，但本质上是$\epsilon$的二次型。随机误差期望为0，所以本质上只有trace那一部分。

样本结构的方差协方差矩阵？

$\sigma^2tr(A)=\sigma^2tr((X^TX)^{-1}(W_k^*)^{-1}W_k^*)$

用前面那两种不同的$W_k$和$W_k^*$等价形式分别带入。

trace与特征值的和？

![截屏2021-04-22 09.05.11](https://tva1.sinaimg.cn/large/008i3skNgy1gps8wyjhjuj30t807ugmy.jpg)

![截屏2021-04-22 09.07.01](https://tva1.sinaimg.cn/large/008i3skNgy1gps8yw8g0rj30q80ekwgk.jpg)

![截屏2021-04-22 09.13.49](https://tva1.sinaimg.cn/large/008i3skNgy1gps95xxb8hj30tk03cwey.jpg)

### $I_2(k)$

$L=\Lambda_k^2$



![截屏2021-04-22 09.17.08](https://tva1.sinaimg.cn/large/008i3skNgy1gps99fstl5j30tk0gkmzg.jpg)

$\alpha^TL\alpha$可以理解成加了个矩阵，加权求和。

### 求$H'(k)$

![截屏2021-04-22 09.23.28](https://tva1.sinaimg.cn/large/008i3skNgy1gps9fzfw3hj30tk0gkjtm.jpg)

### K不好找

![截屏2021-04-22 09.26.07](https://tva1.sinaimg.cn/large/008i3skNgy1gps9it2dwxj30tk0i641b.jpg)

