# 统计与机器学习W6

我们一共有p个变量

## 全模型

由于共有$p$个自变量纳入模型，我们将由所有$p$个自变量构造的回归模型定义为全模型。

$y=X_p\beta_p+\epsilon$

$X_p=(X_{p_1},Z)$

$\beta_p=\begin{pmatrix}\beta_{p_1}\\\gamma\end{pmatrix}$

## 选模型

这里$p_1$是严格小于$p$的。

$y=X_{p1}\beta_{p1}+\epsilon$

## 选多少个变量与Overfitting/Underfitting与否的相关问题

Overfitting在机器学习中本指过多参数对模型的影响，在这里，线性中，指变量是否引入了模型之中。

i.e. 真实影响x的y到底有几个的问题?

> 全模型为真，我们采用了选模型则为欠拟合；
>
> 选模型为真，我们采用了全模型则为过拟合；

## 欠拟合

### 参数估计$\hat{\beta}$

$\hat{\beta}=(X^TX)^{-1}X^{T}y$

#### 采用全模型的参数估计$\hat{\beta_p}$

$\hat{\beta_p}=(X_p^TX_p)^{-1}X_p^{T}y$

$E[\hat{\beta_{p_1}}]=(X_{p_1}^TX_{p_1})^{-1}X_{p_1}^{T}E[y]$

这里$E[y]$怎么求？

$E[y]=E[X_{p_1}\beta_{p_1}+\epsilon+Z\gamma]$

$E[\hat{\beta_{p_1}}]=(X_{p_1}^TX_{p_1})^{-1}X_{p_1}^{T}(X_{p_1}\beta_{p_1}+Z\gamma)$

$=\beta_{p_1}+(X_{p_1}^TX_{p_1})^{-1}X_{p_1}^{T}Z\gamma$

Generally ，这里后面的这一坨是不为0的。因为$Z\gamma，\gamma$一般不为0(欠拟合一般认为我们丢失了重要的变量的角度去理解）.

但如果$X_{p_1}^{T}Z$为0呢？，这个时候就有无偏估计了呀.

### 参数估计$\hat{\sigma^2}$

$\hat{\sigma^2}=\frac{1}{n-p-1}(e^Te)\frac{1}{n-p-1}SSE$

$SSE=\sum(y-\hat{y})^2=\sum(y-Hy)^2=y^T(I-H_{X_p})y$

这里$H_{X_{p_1}}$是将$X$按照分量拆开的帽子矩阵。

$H_{X_p}=X_p(X_p^TX_p)^{-1}X_p^T$

$X_p=(X_{p_1},Z)$

$X_p^T=(X_{p_1}^T,Z^T)$

$H_{X_p}=(X_{p_1},z)\begin{pmatrix}X_{p_1}^TX_{p_1}&&X_{p_1}^TZ\\Z^TX_{p_1}&&Z^TZ\end{pmatrix}\begin{pmatrix}X_{p_1}^T\\Z^T\end{pmatrix}$

可以通过上面这个等式去寻找$H_{X_p}$和$H_{X_{p_1}}$的差异。

#### 采用全模型的参数估计$\hat{\sigma^2}$

$\hat{\sigma^2_p}=\frac{1}{n-p-1}SSE^p$

#### $SS_E^p$与$SS_E^{p_1}$的关系

$SS_E^p=SS_E^{p_1}+y^T(H_{X_p}-H_{X_{p_1}})y$



### 预测

$\hat{y}=x_0^T\hat{\beta}$

> 预测：对点的预测，就是相当于求期望，然后期望里如果有未知参数则用估计去代。

