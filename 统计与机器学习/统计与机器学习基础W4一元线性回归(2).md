# 统计与机器学习基础W4:一元线性回归(2)

一元线性回归是用统计去拟合一个y最有可能的值。

对于每一个x都有一个确定的y的分布$p(y|x)$。所以我们要找的"y"，其实是$E[Y|x]=f(x)=\int yp(y|x)dy$

线性回归是针对回归系数要求线性。

$y=f(x)+\epsilon$

## 三大基本假设

**勾画出具体的两个变量之间的关系，不做点假设你从哪里下手?**

1. $x$被假定为一般变量，非随机变量，是严格控制或精确测量的。
2. $E[\epsilon]=0,Var(\epsilon)=\sigma^2,$就算期望不为0也会被吸收,Gauss-Markov条件。
3. $\epsilon$~i.i,d $N(0,\epsilon^2)$，正态分布假定 ;$E[\epsilon]=0$保证了我们回归函数的形式，是我们研究的根基。假定成正态分布，它让回归“有办法”能够捕获到“概率最大”的点。

## 一元线性回归的基本形式

$y_i=\beta_0+\beta_1x_i+\epsilon_i$

$y_i$～$N(\beta_0+\beta_1x_i,\sigma^2)$

注意不要忘了$y_i$本身也是一个正态分布！

## 估计系数

### LSE & MLE

LSE: min $\sum(y_i-(\beta_0+\beta_1x_i))^2$,

怎么求这个误差的最小值呢，就是对$\beta_0$和$\beta_1$分别求偏导:

注意这里求偏导得到的两个式子:

![截屏2021-03-29 20.23.38](https://tva1.sinaimg.cn/large/008eGmZEly1gp11ngz2w0j30gq06ymxu.jpg)

$\hat{\beta_1}=\frac{l_{xy}}{l_{xx}}$ 本质上就是$y_1,y_2,...,y_n$的线性组合

$\hat{\beta_0}=\bar{y}-\beta_1\bar{x}$；

对于$\hat{\beta_1}=\frac{l_{xy}}{l_{xx}}$,需要不要把概念忘掉的是，x在这里永远是确定的！所以在分子上，把$y_i-y$需要进行拆开。

$\hat{\beta_1}=\frac{\sum(x_i-\bar{x})y_i}{l_{xx}}-\frac{\sum(x_i-\bar{x})\bar{y}}{l_{xx}}=\frac{\sum(x_i-\bar{x})y_i}{l_{xx}}-0$

$\sum(x_i-\bar{x})$永远是0呀不要忘了！

由这个拆看，可以看出$\beta_1$是线性组合。

要估计$\sigma^2$，可以把$\sigma^2$当作一个变量去考虑。

![截屏2021-03-29 20.18.58](https://tva1.sinaimg.cn/large/008eGmZEly1gp11ipwns1j30xw09kq4s.jpg)

### 无偏性--$\hat{\beta_1}$

$E[\hat{\beta_1}]=\sum\frac{x_i-\bar{x}}{l_{xx}}(\beta_0+\beta_1x_i)$

$=\sum\frac{(x_i-\bar{x})\beta_1x_i}{l_{xx}}$,同样还是应用了$\sum(x_i-\bar{x})$永远是0呀不要忘了！

$\sum(x_i-\bar{x})x_i=\sum x_i^2-\bar{x}\sum x_i=\sum x_i^2-\bar{x}n\bar{x}=\sum x_i^2-n\bar{x}^2$

而上学期的统计基础也相应地学过,$\sum x_i^2-n\bar{x}^2=\sum(x_i-\bar{x})^2$

所以$E[\hat{\beta_1}]=\sum\frac{x_i-\bar{x}}{l_{xx}}(\beta_0+\beta_1x_i)$

=$\beta_1\sum\frac{x_i^2-n\bar{x}^2}{\sum x_i^2-n\bar{x}^2}$

$=\beta_1$【这个等式到这个等式的推导还是好神奇...

无偏性--$\hat{\beta_0}$

$\hat{\beta_0}=\bar{y}-\hat{\beta_1}\bar x$

$E[\hat{\beta_0}]=E[\bar{y}]-E[\hat{\beta_1}\bar x]$

注意这里要搞清谁是要求期望的随机变量,所以才能把$\bar{x}$提出来呀。

提出来。

=$\frac{1}{n}\sum E[y_i]-E[\hat{\beta_1}]\bar{x}$

注意这里出错了,$E[y_i]=\beta_0+\beta_1x_i$，不是外面再套一个期望。

=$\frac{1}{n}\sum (\beta_0+\beta_1x_i)-\beta_1\bar{x}$

=$\frac{1}{n}\sum(\beta_0+\beta_1x_i)-\beta_1\bar{x}$

=$\frac{1}{n}\sum(\beta_0+\beta_1x_i)-\beta_1\bar{x}$

=$\frac{1}{n}(n\beta_0+\beta_1\sum x_i)-\beta_1\bar{x}$

## 残差关系式

因为估计完$\beta_0$和$\beta_1$，残差$e_i=y_i-\hat{y_i}=y_i-(\hat{\beta_0}+\hat{\beta_1}x_i)$就可以使用估计去替代了！

根据OLSE求偏导的式子，可以得到$\sum e_i=0$和$\sum x_ie_i=0$

### 方差：

注意每一个$y_i$之间的协方差为0.





## 回归系数检验方法

知道多种检验方法可以让我们使用很少的已知数据得到我们想要的结果，防止因为数据量的缺失而使计算无法进行。

$\beta_1=0$ v.s.	$\beta_1\neq0$

### t检验

求$\hat{\beta_1}$的分布，$\beta_1$~$N(0,\frac{\sigma^2}{l_{xx}})$

残差:

$e_i=y_i-\beta_0-\beta_1x_i$也服从正态分布

$\hat{\sigma^2}=\frac{1}{n-2}\sum e_i^2$；残差平方和$SSE$

不加证明的说明残差服从自由度$n-2$的卡方分布。

所以有正太和卡方可以构造t分布

$\frac{\hat{\beta_1}}{\hat{\sigma^2}/l_{xx}}$～$t(n-2)$

### F检验

$sst=sse+ssr$

$ss_T=l_{yy}=\sum(y_i-\bar{y})^2$

$SSE=(y_i-\hat{y_i})$残差平方和

$SSR=(\hat{y_i}-\bar{y})^2$叫回归平方和

![[公式]](https://www.zhihu.com/equation?tex=SSR) 服从自由度为1的卡方分布，而 ![[公式]](https://www.zhihu.com/equation?tex=SSE) 服从自由度为 ![[公式]](https://www.zhihu.com/equation?tex=n-2) 的卡方分布（上面已经见过它了）。

$\frac{ssR/1}{SSE/(n-2)}$~$F(1,n-2)$

> 矩阵在回归分析中的应用

### 相关系数

$r=\frac{l_{xy}}{\sqrt{l_{xx}l_{yy}}}$

$r^2=\frac{l_{xy}^2}{l_{xx}l_{yy}}$,$l_{yy}=SST$

$ssR=\sum(\hat{y_i}-\bar{y})^2=\hat{\beta_1}^2l_{xx}$

$ssr/sst=lxy^2 lxx/lxx^2/lyy=(lxy)^2/lxxlyy=r^2$

统计学上$ssr/sst$为决定系数等于$r^2$

