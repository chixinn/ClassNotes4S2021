# 统计与机器学习基础W3：一元线性回归

Simple linear Regression

影响y的因素的数量(n元)

多个y叫多重线性回归；元：x(multiple)；重:y(multivariate)

> 方差分析和回归都是线性模型，在验证其他好不好要先验证这个，所有方法的baseline

Box-plot

## 1.来源故事

高尔顿身高实验，遗传学回归现象

"回归现象"：孩子的平均身高会向中心线回归，即回归现象(父母身高的线的斜率比孩子身高的线的斜率大)

- Step1:散点图--配直线

  $\hat{y}=33.75+0.516x$

  回归方程：

  - x每增加一个单位，y相应的增加0.516个单位(不是0.516+33.75!!)
  - 这里0.516<1，恰好可以解释回归，即身高特性不会极端化，而是会平均发展

  - Regress to a line.

两个变量之间的关系

1. 确定性关系

   $S=a^2/V=IR$

   不确定：相同人身高的不同腿长，统计做数据都是研究不确定

2. 相关关系

## 2. 模型及假设

### 模型

$y$(响应变量 dependent variable)$=f(x)$(协变量/自变量 independent variable)+$\epsilon$(随机误差)

前一部分$f(x)$是**确定性**的，后面误差是**随机性**的

存在随机性现象下的确定性的关系。

回归:配直线$f(x)=\beta_0+\beta_1x+\epsilon$

### 数据

$(x_i,y_i),I=1,2,3,...,n$，n为样本量

$y_i=\beta_0+\beta_1x_i+\epsilon_i$

- 假设1:$E[\epsilon_i]=0$,如果不为0，完全可以和$\beta_0$合并，$Var(\epsilon_i)<\infin$，独立性。
- 假设2:$\epsilon_i$ i.i.d~$N(0,\sigma^2)$

## 3.参数估计及性质

### LSE(最小二乘估计 Least Square Estimator)

$(\hat{\beta_0^{LSE}},\hat{\beta_1^{LSE}})=argmin_{\beta_0,\beta_1}(\sum_i^n(y_i-\beta_0-\beta_1x_i)^2)=Q(\beta_0,\beta_1)$

### 怎么求

正规方程组

$\frac{\part{Q}}{\part\beta_0}=-2\sum(y_i-\beta_0-\beta_1x_i)=0$<=>$\sum y_i-n\beta_0-\beta_1\sum x_i=0$

$\frac{\part{Q}}{\part\beta_1}=-2\sum(y_i-\beta_0-\beta_1x_i)x_i=0$<=>$\sum y_ix_i-\beta_0\sum x_i-\beta \sum x_i^2=0$

$l_{xx}=\sum(x_i-\bar{x})^2;l_{yy}=sum(y_i-\bar{y})^2;$

$l_{xy}=\sum(x_i-\bar{x})(y_i-\bar{y})=\sum (x_iy_i -\bar{y}x_i-\bar{x}y_i+\bar{y}\bar{x})$

$\bar{x},\bar{y}$与x和y无关

所以，$n\bar{y}=\sum y_i$

$\hat{\beta_1^{LSE}}=\frac{l_{xy}}{l_{xx}}$;

$\hat{\beta_0^{LSE}}=\bar{y}-\hat{\beta_1}\bar{x}$

### MLE

1. 构造似然函数

假设：$\epsilon_i$ i.i.d~$N(0,\sigma^2)$，$y_i$~$N(\beta_0+\beta_1x_i,\sigma^2)$,$y_i$之间独立。

因为$y_i$之间**独立性**存在，$L(\beta_0,\beta_1)=f(y_1,y_2,..,y_n)=\Pi f(y_i)$(后面这个第二个存在是因为独立性条件的)

> 数据处理的时候要注意看数据是否独立！
>
> 如时序数据分析等，要注意是否有独立性

$max [L(\beta_0,\beta_1)]=max [exp-\frac{1}{2\sigma^2}(y_i-\beta_0-\beta_1x_i)]=max [Q(\beta_0,\beta_1)]$

**所以，MLE和LSE推出来是一样的。**

**所以两个估计的求解就不做区分了。所以上面的上角标LSE就无关了**

### 比较估计的好坏

相合性

#### 定理

在上面假设2($\epsilon$是正态的条件下，$y_i$~$N(\beta_0+\beta_1x_i,\sigma^2)$

- $\hat{\beta_0}$~$N(\beta_0,(\frac{1}{n}+\frac{\bar x^2}{l_{xx}})\sigma^2)$

​      $\hat{\beta_1}$~$N(\beta_1,\frac{\sigma^2}{l_{xx}})$

- $Cov(\beta_0,\beta_1)=-\frac{\bar{x}}{l_{xx}}\sigma^2$

- 拟合值，给定$x_0$，$y_0=\beta_0+\beta_1x+\epsilon$

  $\hat{y_0}=\hat{\beta_0}+\hat{\beta_1}x_0$~$N(\beta_0+\beta_1x_0,(\frac{1}{n}+\frac{(x-\bar x)^2}{l_{xx}})\sigma^2)$

#### 证明

$\hat{\beta_1}=\sum\frac{(x_i-\bar{x})}{l_{xx}}y_i=(\frac{(x_1-\bar{x})}{l_{xx}},\frac{(x_2-\bar{x})}{l_{xx}},....,)【A】\begin{pmatrix}y_1\\y_2\\...\\y_n\end{pmatrix}【Y】$

正态性是由写成线性变换的结构保证的。$AY$

$\sum(x_i-\bar{x})=0$

$\hat{\beta_0}=\sum(\frac{1}{n}-\frac{\bar x(x_i-\bar x)}{l_{xx}})$

> 重要定理
>
> $Y$~$N(\vec{\mu_y},\Sigma_y)$
>
> $AY$~$N(A\vec{\mu_y},A\Sigma_yA^T)$,这里A可以是一个降维矩阵

#### 区别

最小二乘和极大似然的本质思想有差异

---

## 定理1

在模型$\begin{cases}y_i=\beta_0+\beta_1x_i+e_i&&I=1,2,3...,n\\e_i N(0,\sigma^2)I,i,d\end{cases}$

$(1)\hat{\beta_0}$~$N(\beta_0,(\frac{1}{n}+\frac{\bar x^2}{l_{xx}})\sigma^2)$， $\hat{\beta_1}$~$N(\beta_1,\frac{\sigma^2}{l_{xx}})$

$(2)Cov(\beta_0,\beta_1)=-\frac{\bar{x}}{l_{xx}}\sigma^2$

$(3)$给定$x_0,y_0，y_0=\beta_0+\beta_1x_0+e_0,\hat{y_0}=\hat{\beta_0}+\hat{\beta_1}x_0$~$N(\beta_0+\beta_1x_0,(\frac{1}{n}+\frac{(x_0-\bar x)^2}{l_{xx}})\sigma^2)$

> $y_0$是随机变量，$\beta_0+\beta_1x_0$是参数，$\hat{y_0}$是参数的无偏估计。



$Cov(x_1+x_2,Y)=Cov(x_1,Y)+Cov(x_2+Y)$

$\hat{\beta_1}=\frac{l_{xy}}{l_{xx}}=\frac{\sum(x_i-\bar{x})(y_i-\bar{y})}{l_{xx}}= \frac{\sum(x_i-\bar{x})y_i}{l_{xx}}-\frac{\sum(x_i-\bar{x})\bar{y}}{l_{xx}}=\frac{\sum(x_i-\bar{x})y_i}{l_{xx}}-0$

$\hat{\beta_0}=\bar{y}-\hat{\beta_1}\bar{x}=\frac{1}{n}\sum y_i-\frac{\sum\bar{x}(x_i-\bar{x})y_i}{l_{xx}}$,这里按$y_i$整理，=$\sum(\frac{1}{n}-\frac{\bar{x}(x_i-\bar{x})}{l_{xx}})y_i$

$Cov(,)=E[xy]-E[x]E[y]$

一系列相互独立的随机变量的加权和，只有在相互时有Cov,不相互是独立的，Cov为0

在x给定的情况下y的情况，所以x是参数？

$\sum(x_i-\bar{x})$

为什么这里只有$y_i$是随机变量，$x_i$是固定的？没有随机性

## 如何提高$\hat{\beta_0}$的估计精度？

提高精度-》方差降低。

1. 提高样本量
2. $l_{xx}$足够大，x足够宽，即使x尽可能的分散
3. 误差的方差$\sigma^2$

## 回归方程

$\hat{y}=\hat{\beta_0}+\hat{\beta_1}x$

> 可以用回归方程刻画，x和y的关系吗
>
> 等价于
>
> 是否真的存在线性关系?
>
> $y=\beta_0+\beta_1x+e$

所以很自然的$H_0:\beta_1=0;H_1:\beta_1\neq0$

## 4.显著性检验

上述问题为检验问题

### 法1:F检验

构造F统计量

总偏差分解定理：模型与平均水平，每个点与模型之间的差异？

$\sum(y_i-\bar{y})^2=\sum(\hat{y_i}-\bar{y})^2+\sum(y_i-\hat{y_i})^2$=SSR+SSE

给予这个东西，会有一个假设检验

$T=\frac{SSR/1}{SSE/(n-2)}$

在原假设成立，SSR会非常小，那么检验统计量会很小；

$T>C$拒绝。



## 定理2:上述检验统计量T为什么合理

$E（SSR）=\sigma^2+\beta_1^2l_{xx}$

$\bar{y}=\beta_0+\beta_1\bar{x}$

$E[SSE]=(n-2)\sigma^2$

$Cov(\hat{\beta_0},e_i)$对于第i项而言误差和观测是不独立的，$\hat{\beta_0}=\bar{y}-\hat{\beta_1}\bar{x}$，在这里把$\hat{\beta_0}$拆开

训练集的估计值和训练集中的误差；不在trainingset的项误差是独立的。

## 如何求T>c中的c?

求C，即是去找SSR和SSE的分布，

既然是F检验，所以SSR和SSE的分布应该是独立的卡方。

> 从加权求和角度看估计量

## 定理3:

设$y_1,y_2,...,y_n$相互独立，且$y_i$~$N(\beta_0+\beta_1x_i,\sigma^2)$

- SSE/$\sigma^2$~$x^2(n-2)$
- H_0下，SSR~$X^2(1)$
- SSR ,SSE ,$\bar{y}$相互独立($\hat{\beta_1},SSE,\bar{y}$)相互独立

证明：

去构造一个正交矩阵，其逆和转置相同，任意两行线性无关。

第一行和最后一行；

第一行和倒数第二行；

第一行和第一行(自身内积)

第一行 和第二行



未知参数个数>约束，说明这个方程一定解存在(无穷多组解)，即论证了A是存在的。

## 5.置信区间v.s. 预测区间

