# 统计与机器学习基础W3：一元线性回归

Simple linear Regression

影响y的因素的数量(n元)

多个y叫多重线性回归；元：x(multiple)；重:y(multivariate)

> 方差分析和回归都是线性模型，在验证其他好不好要先验证这个，所有方法的baseline

Box-plot

## 1.来源故事

高尔顿身高实验，遗传学回归现象

"回归现象"：孩子的平均身高会向中心线回归，即回归现象(父母身高的线的斜率比孩子身高的线的斜率大)

- Step1:散点图--配直线

  $\hat{y}=33.75+0.516x$

  回归方程：

  - x每增加一个单位，y相应的增加0.516个单位(不是0.516+33.75!!)
  - 这里0.516<1，恰好可以解释回归，即身高特性不会极端化，而是会平均发展

  - Regress to a line.

两个变量之间的关系

1. 确定性关系

   $S=a^2/V=IR$

   不确定：相同人身高的不同腿长，统计做数据都是研究不确定

2. 相关关系

## 2. 模型及假设

### 模型

$y$(响应变量 dependent variable)$=f(x)$(协变量/自变量 independent variable)+$\epsilon$(随机误差)

前一部分$f(x)$是**确定性**的，后面误差是**随机性**的

存在随机性现象下的确定性的关系。

回归:配直线$f(x)=\beta_0+\beta_1x+\epsilon$

### 数据

$(x_i,y_i),I=1,2,3,...,n$，n为样本量

$y_i=\beta_0+\beta_1x_i+\epsilon_i$

- 假设1:$E[\epsilon_i]=0$,如果不为0，完全可以和$\beta_0$合并，$Var(\epsilon_i)<\infin$，独立性。
- 假设2:$\epsilon_i$ i.i.d~$N(0,\sigma^2)$

## 3.参数估计及性质

### LSE(最小二乘估计 Least Square Estimator)

$(\hat{\beta_0^{LSE}},\hat{\beta_1^{LSE}})=argmin_{\beta_0,\beta_1}(\sum_i^n(y_i-\beta_0-\beta_1x_i)^2)=Q(\beta_0,\beta_1)$

### 怎么求

正规方程组

$\frac{\part{Q}}{\part\beta_0}=-2\sum(y_i-\beta_0-\beta_1x_i)=0$<=>$\sum y_i-n\beta_0-\beta_1\sum x_i=0$

$\frac{\part{Q}}{\part\beta_1}=-2\sum(y_i-\beta_0-\beta_1x_i)x_i=0$<=>$\sum y_ix_i-\beta_0\sum x_i-\beta \sum x_i^2=0$

$l_{xx}=\sum(x_i-\bar{x})^2;l_{yy}=sum(y_i-\bar{y})^2;$

$l_{xy}=\sum(x_i-\bar{x})(y_i-\bar{y})=\sum (x_iy_i -\bar{y}x_i-\bar{x}y_i+\bar{y}\bar{x})$

$\bar{x},\bar{y}$与x和y无关

所以，$n\bar{y}=\sum y_i$

$\hat{\beta_1^{LSE}}=\frac{l_{xy}}{l_{xx}}$;

$\hat{\beta_0^{LSE}}=\bar{y}-\hat{\beta_1}\bar{x}$

### MLE

1. 构造似然函数

假设：$\epsilon_i$ i.i.d~$N(0,\sigma^2)$，$y_i$~$N(\beta_0+\beta_1x_i,\sigma^2)$,$y_i$之间独立。

因为$y_i$之间**独立性**存在，$L(\beta_0,\beta_1)=f(y_1,y_2,..,y_n)=\Pi f(y_i)$(后面这个第二个存在是因为独立性条件的)

> 数据处理的时候要注意看数据是否独立！
>
> 如时序数据分析等，要注意是否有独立性

$max [L(\beta_0,\beta_1)]=max [exp-\frac{1}{2\sigma^2}(y_i-\beta_0-\beta_1x_i)]=max [Q(\beta_0,\beta_1)]$

**所以，MLE和LSE推出来是一样的。**

**所以两个估计的求解就不做区分了。所以上面的上角标LSE就无关了**

### 比较估计的好坏

相合性

#### 定理

在上面假设2($\epsilon$是正态的条件下，$y_i$~$N(\beta_0+\beta_1x_i,\sigma^2)$

- $\hat{\beta_0}$~$N(\beta_0,(\frac{1}{n}+\frac{\bar x^2}{l_{xx}})\sigma^2)$

​      $\hat{\beta_1}$~$N(\beta_1,\frac{\sigma^2}{l_{xx}})$

- $Cov(\beta_0,\beta_1)=-\frac{\bar{x}}{l_{xx}}\sigma^2$

- 拟合值，给定$x_0$，$y_0=\beta_0+\beta_1x+\epsilon$

  $\hat{y_0}=\hat{\beta_0}+\hat{\beta_1}x_0$~$N(\beta_0+\beta_1x_0,(\frac{1}{n}+\frac{(x-\bar x)^2}{l_{xx}})\sigma^2)$

#### 证明

$\hat{\beta_1}=\sum\frac{(x_i-\bar{x})}{l_{xx}}y_i=(\frac{(x_1-\bar{x})}{l_{xx}},\frac{(x_2-\bar{x})}{l_{xx}},....,)【A】\begin{pmatrix}y_1\\y_2\\...\\y_n\end{pmatrix}【Y】$

正态性是由写成线性变换的结构保证的。$AY$

$\sum(x_i-\bar{x})=0$

$\hat{\beta_0}=\sum(\frac{1}{n}-\frac{\bar x(x_i-\bar x)}{l_{xx}})$

> 重要定理
>
> $Y$~$N(\vec{\mu_y},\Sigma_y)$
>
> $AY$~$N(A\vec{\mu_y},A\Sigma_yA^T)$,这里A可以是一个降维矩阵

## 4.显著性检验



## 5.置信区间v.s. 预测区间

