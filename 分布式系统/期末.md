# 分布式计算系统

我们的卷子，选择题，填空题，大题；选择填空的话，可能会考一题细节的东西，比如hadoop项目还是Flink啥的（记不太得）的前身是啥之类的，然后就是一些知识点，split，block啥的那些；大题的话，就可能会画架构图，然后分析一些东西之类的，比如用map reduce实现迭代和spark实现迭代的差别

## 批流融合基础

### 1. Lambda架构优缺点

- 优点：平衡了重新计算开销和需求低延迟的矛盾
- 缺点：
  - 开发复杂：
    - 分开编程
  - 运维复杂：同时维护两套执行引擎

### 2. 批处理系统能否处理无界数据集？流处理系统能否处理有界数据集？

批处理系统也可以用于处理流数据；流计算系统也可以用于处理批数据，只不过性能无法达到预期。

例如spark Streaming就是用底层的spark批处理引擎来处理流数据。

### 3. 事件时间和处理时间

- 事件时间：该事件实际发生的事件(所在系统，可能是传感器等数据源)的当前时间
- 处理时间：一个事件在数据处理的过程中被数据处理系统观察到的时间。
- 事件时间永远不变，但是处理时间随着该记录在系统中被各个节点处理时而持续变化。

### 4. 水位线的含义是什么？水位线机制用于解决什么问题

解决问题：衡量处理时间和事件时间之间的差值。

含义：事件时间戳，

![Screen Shot 2021-06-27 at 10.22.48](分布式计算系统.assets/Screen Shot 2021-06-27 at 10.22.48.png)

### 5.ParDo和GroupByKey实现列表求和



## Flink

- Stratosphere:批处理系统引入流水线方式进行数据传输
- 柏林大数据中心

### 设计思想

以刘计算为核心，将有界视为无界的特例

#### 数据模型

|       | 数据模型   | 有界无界 | 可变不可变 | 集合级别 |
| ----- | ---------- | -------- | ---------- | -------- |
| MR    | <k,v>      | 有界     |            | 记录     |
| Flink | DataStream | 无界     | 不可变     | 集合     |
| Spark | RDD        | 有界     | 不可变     | 集合     |
| Storm | Turple     | 无界     |            | 记录     |

#### 计算模型

- DataSource
- Transformation
- DataSink:标志着DAG的结束，Spark中Action表示最后一个

##### 逻辑计算模型

Spark逻辑计算模型：1. Operator DAG 2. RDD Lineage

Flink逻辑计算模型:Operator DAG 

> 为什么Flink没有RDD Lineage

##### 物理计算模型

#### 迭代模型

- MR：每一次迭代结束，结果写入HDFS，下次再将该结果读出。**迭代次数决定了MapReduce作业的个数。**
- Spark：每一次迭代结束生成一个RDD，就是下次的输入，spark迭代的RDD数据可以直接驻留在内存中。整个迭代可以是一个作业。整个迭代是一个spark应用
- Flink: 系统提供迭代算子(编程时不需要使用for/while循环)。整个迭代可以是一个作业。

### 体系架构

客/主/从

- Client:Flink程序->逻辑执行图(DAG)->**优化**后的逻辑执行图(Chaining优化，将窄依赖算子合并起来形成一个大算子).
- JobManager(主节点):优化后的逻辑执行图->根据算子的并行度，物理执行图。物理执行图中的一个结点对应一个任务，将分配给TaskManager来执行。
- TaskManager

#### Standalone模式

![Screen Shot 2021-06-27 at 09.38.19](分布式计算系统.assets/Screen Shot 2021-06-27 at 09.38.19.png)

- StandaloneSessionClusterEntrypoint: Flink系统的资源管理。
- TaskManagerRunner: 所在节点的资源管理/执行任务：将资源抽象成若干个TaskSlot用于任务的执行。

#### 对比Spark的Master/Worker

![Screen Shot 2021-06-27 at 09.44.50](分布式计算系统.assets/Screen Shot 2021-06-27 at 09.44.50.png)

![Screen Shot 2021-06-27 at 09.47.11](分布式计算系统.assets/Screen Shot 2021-06-27 at 09.47.11.png)

![Screen Shot 2021-06-27 at 09.47.30](分布式计算系统.assets/Screen Shot 2021-06-27 at 09.47.30.png)

#### yarn模式

![Screen Shot 2021-06-27 at 11.43.52](分布式计算系统.assets/Screen Shot 2021-06-27 at 11.43.52.png)

### 工作原理

![Screen Shot 2021-06-27 at 11.57.34](分布式计算系统.assets/Screen Shot 2021-06-27 at 11.57.34.png)

> 5. Flink中的逻辑执行图和物理执行图分别是如何产生的？与Spark有何区别？
>
> 逻辑执行图：Client:用户程序->DAG->经过Chaining优化后的DAG；
>
> 物理执行图:  JobManager收到Client提交的逻辑执行图，将逻辑执行图转化为物理执行图。
>
> 而spark中，是由driver自己完成了从用户程序到DAG，再从DAG到物理执行图的转换。

#### 算子任务分配的原则：

- 考虑local(计算向数据靠拢)
- 数据shuffle的量要减少

#### 非迭代任务之间的数据传输

- shuffle是一种阻塞的数据传输的方式(map和reduce之间)，spark不同stage之间，位于上游的任务必须等到所有的记录都计算结束后再向下游任务传递数据。
- 消息传递是一种非阻塞的数据传输方式。
- **如果实现非迭代算子的任务位于不同的TaskManager**,那么Flink将采用流水线机制进行数据交换。
- Flink 流水线机制一次传递一个buffer.

> 6. Flink中非迭代任务间如何进行数据传输？与spark和storm有什么区别？
>
>    流水线机制，非阻塞，上游buffer满了或超时即输出到下游。
>
>    spark位于不同stage之间采用shuffle传输，shuffle是阻塞式的，必须等到上游所有的记录都计算结束后再向下游传递数据。
>
>    flink流水线机制和storm的区别在于，虽然二者都是非阻塞式的，但是数据传输的粒度不同，flink采用buffer，集合级别是集合，storm采用消息传递，一次传递一条记录。

#### Spark Pipeline v.s. Flink  Pipeline

- Flink  Pipeline:不同Task之间的数据传输方式
- Spark：同一个Task实现多个不同算子。

#### 为什么Flink不能采用Lineage进行故障恢复？

Spark由于采用阻塞式的数据传输方式，根据一个完整的RDD计算得出另一个完整的RDD，因为RDD生成也是阻塞式的。Flink中的数据发送条件实际相当于一个不完整的DataStream产生另一个不完整的DataStream并且DataStream中的记录不断的发生变化。

**Flink DataStream中数据的不完整和不断变化的特点导致了无法像RDD那样支持DataStream的持久化**，Flink系统也无法像Spark那样利用RDD Lineage进行故障恢复。

#### 迭代任务的数据传输

8. flink中迭代算子如何实现数据反馈？

迭代算子内部存在数据反馈的环路。

两类特殊的任务：迭代前端和迭代末端两类特殊的任务，同处于同一个TaskManager。

流式迭代的数据传输-每一轮迭代计算的部分结果：

迭代前端下一轮计算**不依赖**迭代末端在前一轮迭代得到的**所有**记录。

因为不依赖所有，所以就可以一部分，一部分这样:D

- 一部分：作为输出向后传递
- 一部分：另一部分的结果作为下一轮迭代计算的输入。

因为不用等待所有记录，所以流水线方式数据传输。

### 容错机制

“一满就传”

#### 异步屏障快照算法

快照：检查点

通过在输入数据中注入屏障，并异步地保存快照，达到和保存同一时刻所有算子状态的目的。

屏障对齐：一个任务需收到来自上游任务中所有标识为n的屏障后才将其状态保存起来。

异步：某一任务将标识为n的屏障对齐后，可继续接受属于检查点n+1的数据。

故障恢复：选择最近完整的检查点n，将系统中每个算子的状态重置为检查点中保存的状态。

准确一次的容错语义

如果是迭代计算的容错，反馈环路中的所有记录都需要以日志的形式保存起来。

#### 状态管理

## Spark

1. spark由UCBerkeley的AM片实验室开发
2. 基于内存计算->内外存同时使用。

### 1.MR的局限性（编程框架/作业执行/系统架构)

编程框架和作业执行

- 编程框架的表达能力有限
- 单个作业shuffle，阻塞传输，IO开销大
- 多个作业之间衔接涉及IO开销。

---

系统架构

- 资源管理和作业管理的紧密耦合(Hadoop1.0)
- 作业控制管理高度集中(Job Tracker单点故障)

![Screen Shot 2021-06-27 at 18.22.41](分布式计算系统.assets/Screen Shot 2021-06-27 at 18.22.41.png)

### 数据模型--RDD(高度受限的共享内存)

#### 操作算子

- 创建
- 转换
- 行动action:一个RDD只能有一个action

#### 逻辑计算模型

- Operator DAG：从算子操作角度来描述计算的过程，主体是算子
- RDD Lineage：从RDD变换的角度来描述计算过程，主体是数据

#### 物理计算模型

DAG中的操作算子实际上由若干个实例任务(Task)来实现

#### 2. 物理计算模型和逻辑计算模型的关系

逻辑计算模型是由创建算子、转换算子和行动算子所组成的一个DAG；物理计算模型是指整个DAG在物理(Spark集群)需要启动若干个实例任务(Task)来实现。

#### 3. RDD Lineage，如果RDD Lineage较长，如何加快故障恢复

RDD Lineage是DAG通过拓扑排序的结果。

当RDD Lineage存在大量宽依赖，恢复故障过程的代价较高。因此，可以通过在适当的时机设置检查点以免发生故障后恢复的代价过高。较长的Lineage广泛出现在迭代计算中，所以，利用检查点机制周期性地将某些轮次的迭代计算结果存入检查点。

### 体系架构

#### 抽象架构

![Screen Shot 2021-06-27 at 15.48.50](分布式计算系统.assets/Screen Shot 2021-06-27 at 15.48.50.png)

- Executor：运行在WorkNode的一个进程，负责运行Task
- Task：运行在Executor上的工作单元,线程；MR中的Task是进程。线程间切换的开销总小于进程间切换的开销。
- Cluster Manager【集群资源管理】
- WorkerNode
- Driver【作业管理】
- Executor

> 集群资源管理和作业管理的解耦

#### Standalone

- Cluster Manager【集群资源管理】:Master/Worker
- Driver：逻辑上Driver独立于主节点、从节点以及客户端
- Executor: CoarseGrainedExecutorBackend
- WorkerNode

![Screen Shot 2021-06-27 at 15.58.46](分布式计算系统.assets/Screen Shot 2021-06-27 at 15.58.46.png)

#### Standalone -Client

- Driver和客户端以同一个进程存在

![Screen Shot 2021-06-27 at 18.14.41](分布式计算系统.assets/Screen Shot 2021-06-27 at 18.14.41.png)

#### Standalone-Cluster

- 随机选择从节点某一Worker启动DriverWrapper

![Screen Shot 2021-06-27 at 16.07.20](分布式计算系统.assets/Screen Shot 2021-06-27 at 16.07.20-4781243.png)

### 工作原理

#### Driver

由Driver创建 SparkContext，向Cluster Manager进行资 源申请，并由Driver进行任务分配和监控

Driver:应用程序->逻辑执行图->物理执行图：DAG->Stage

> 多少个sparkContext,多少个application

![Screen Shot 2021-06-27 at 16.15.10](分布式计算系统.assets/Screen Shot 2021-06-27 at 16.15.10.png)

#### 5. 如何划分Stage

通过分析各个RDD中的分区之间的依赖关系来决定如何划分Stage

- DAG的划分者：Driver
- 将窄依赖尽可能划分在同一个Stage,Stage由Task组成，Stage中Task的数量由输出RDD中的分区个数决定

#### Stage内部数据传输

Shuffle 流水线(pipeline)

![Screen Shot 2021-06-27 at 16.22.15](分布式计算系统.assets/Screen Shot 2021-06-27 at 16.22.15.png)

Shuffle 流水线(pipeline) v.s MapReduce Shuffle

mapReduce中shuffle不同，流水线方式不要求物化前序算子的所有计算结果。

#### Stage之间数据传输

Stage之间的数据传输需要进行Shuffle。

在Shuffle Write阶段，ShuffleMapTask需要将输出RDD的记录按照partition函数划 分到相应的bucket当中并物化到本地磁盘 形成ShuffleblockFile，之后才可以在 Shuffle Read阶段被拉取。

#### Application/Job/Task

- Application:多少个sparkcontext就有多少个application
- Job:多少个action就有多少个job
- 应用程序:main方法
- stage是job的基本调度单位
- Task是运行在executor上的线程工作单元
- Job=DAG
- Stage=TaskSet

![Screen Shot 2021-06-27 at 17.26.53](分布式计算系统.assets/Screen Shot 2021-06-27 at 17.26.53.png)

![Screen Shot 2021-06-27 at 17.27.53](分布式计算系统.assets/Screen Shot 2021-06-27 at 17.27.53.png)

### 容错

#### RDD持久化缓存---提高迭代效率/备份容错

RDD不可改，新的RDD很占内存嘛

持久化后的RDD将会被保留在工作节点中被后面的行动操作重复使用。

Spark程序每一次迭代得到一个RDD，该RDD可以驻留在内存中直接作为下一次迭代的输入，避免了冗余的读写开销。此外，对于在迭代过程中保持不变的静态数据，可以利用持久化机制将其缓存在内存。

缓存到内存/HDFS

#### Spark广播变量的机制

c.f. MR分布式缓存(它是在作业执行前，将需要缓存的文件路径分发到各个执行任务中，各个任务可以在执行的过程中通过该缓存的文件路径来获取数据)

将"小表"广播出去，避免"大表"shuffle。

spark的combine机制也可以减少shuffle的数据量，从而提升程序性能。

![Screen Shot 2021-06-27 at 21.41.36](分布式计算系统.assets/Screen Shot 2021-06-27 at 21.41.36-4801301.png)

#### 基于RDD 的Lineage恢复

#### 检查点 v.s. RDD持久化

相同：一定程度的多备份

不同：

- 检查点:检查点机制将RDD写入可靠的外部分布式文件系统如HDFS
- RDD持久化：在spark内部某些节点存储多个备份

## Yarn

> Yarn是资源管理系统，不做计算，所以目录中没有计算模型的介绍

### 1. 对于MR 和Spark来说，应用与作业是否有区别?

详见spark

MR 中不存在action ，所以不在MR中强调Application和Job的差异

### 2.Hadoop2.0和Hadoop1.0相比的优势

![Screen Shot 2021-06-27 at 18.25.37](分布式计算系统.assets/Screen Shot 2021-06-27 at 18.25.37.png)

### 3. spark架构和yarn架构在设计思想上有无共同点?

资源管理和作业管理的解耦

### 4. Yarn的主要部件和各个部件的作用

![Screen Shot 2021-06-27 at 18.42.03](分布式计算系统.assets/Screen Shot 2021-06-27 at 18.42.03.png)

### Yarn应用

![Screen Shot 2021-06-27 at 18.45.03](分布式计算系统.assets/Screen Shot 2021-06-27 at 18.45.03.png)

> Spark中一个应用就是Yarn中的一个应用
>
> Yarn中进行资源分配的对象是应用
>
> 所以spark里一个应用里的多个job，这些job和job之间的资源分配是由spark自己决定的

### 体系架构

- RM：负责整个系统的资源管理和分配的资源管理器
  - Resource Scheduler:
  - Application Manager

8. NM是否监控Container中人物的运行情况

- NM(Container里运行啥与NM无关) 

- AM:每当用户提交一个框架应用，Yarn均启动一个AM用于管理该应用。每当提交一个应用，Yarn都会启动一个对应的ApplicationMaster。

  所以在实验课，提交一个MR作业，其实就相当于一个yarn的应用，再提交一个MR作业，MRAppMaster会增加

- Container：资源的抽象表示

![Screen Shot 2021-06-27 at 19.23.54](分布式计算系统.assets/Screen Shot 2021-06-27 at 19.23.54.png)

![Screen Shot 2021-06-27 at 18.54.34](分布式计算系统.assets/Screen Shot 2021-06-27 at 18.54.34.png)

### 5. Yarn中Container内的任务由谁负责启动/停止?

AM确定资源分配方案后，便与对应的NodeManager通信，在相应的Container中启动相应的工作进程用于执行任务。

### 7. AM申请资源的过程/AM由谁监控/容错恢复的过程

AM容错恢复：重启

AM由NM监控

AM与RM通信协商申请资源

![Screen Shot 2021-06-27 at 19.27.22](分布式计算系统.assets/Screen Shot 2021-06-27 at 19.27.22.png)

![Screen Shot 2021-06-27 at 19.27.33](分布式计算系统.assets/Screen Shot 2021-06-27 at 19.27.33.png)

### 一个平台多个框架

#### Yarn+MR+SparkCluster

![Screen Shot 2021-06-27 at 19.28.11](分布式计算系统.assets/Screen Shot 2021-06-27 at 19.28.11.png)

![Screen Shot 2021-06-27 at 19.35.28](分布式计算系统.assets/Screen Shot 2021-06-27 at 19.35.28.png)

#### Yarn+MR

![Screen Shot 2021-06-27 at 19.29.26](分布式计算系统.assets/Screen Shot 2021-06-27 at 19.29.26.png)

#### Yarn+Cluster+Spark

![Screen Shot 2021-06-27 at 19.29.58](分布式计算系统.assets/Screen Shot 2021-06-27 at 19.29.58.png)

#### Yarn+Client+Spark

![Screen Shot 2021-06-27 at 19.30.23](分布式计算系统.assets/Screen Shot 2021-06-27 at 19.30.23.png)

Executor Laucher仅负责向RM申请资源启动CoarseGrainedExecutorBackend进程，既不负责管理资源也不管理应用，仅负责资源的深情和释放。这里EL就是为了与yarn框架匹配的。与AM一定作为应用管理有一定矛盾。Driver还在客户端嘛

#### Client v.s. Cluster

![Screen Shot 2021-06-27 at 19.30.43](分布式计算系统.assets/Screen Shot 2021-06-27 at 19.30.43.png)

## MapReduce

### MPI v.s. MR

![Screen Shot 2021-06-27 at 20.35.14](分布式计算系统.assets/Screen Shot 2021-06-27 at 20.35.14.png)

### 逻辑计算模型

- Map和Reduce两个过程

### 物理计算模型

分而治之，多个任务并行处理

### 体系架构

![Screen Shot 2021-06-27 at 20.48.52](分布式计算系统.assets/Screen Shot 2021-06-27 at 20.48.52.png)

- JobTracker:资源管理和作业管理(主节点)
- TaskTracker(从节点) 

> TaskTracker如何将自己资源使用情况汇报给JobTracker的？通过HeartBeat
>
> TaskTracker如何衡量资源使用了多少？使用slot

- Task Tracker启动Child进程执行Map任务，启动Child进程执行Reduce任务。在HDFS中，都是child进程，也就是说我们jps看不到mapTask也看不到reduceTask。
- Reduce任务将从**Map任务所在节点的本地磁盘**拉取Map的输出结果。也就是说map的输出在本地，不在hdfs中。

#### MR与HDFS的关系

- 计算与存储相分离
- 计算向数据靠拢而不是数据向计算靠拢。

### 工作原理

只有map任务和reduce任务之间才会通信

5个阶段

HDFS block大小是固定的，可能会存在跨块文件，但是在split之间不会存在跨split文件

#### 数据输入

从分布式文件系统HDFS读取到MR 框架的映射。

#### Map任务的数量

split分片的数量决定。

split其实只是一个逻辑概念，包含数据的一些元数据信息。

#### reduce任务数量

最优的Reduce任务个数取决于集群中可用的reduce任务槽(slot)的数目 •通常设置比reduce任务槽数目稍微小一些的Reduce任务个数(这样可 以预留一些系统资源处理可能发生的错误)

### shuffle

 ![Screen Shot 2021-06-27 at 21.14.05](分布式计算系统.assets/Screen Shot 2021-06-27 at 21.14.05.png)

- MR shuffle c.f.二/多路归并排序
- 每个Reduce任务的输出结果将以一个文件的形式保持到指定的目录当中

### 容错

#### JobTracker

单点瓶颈，重新执行

#### TaskTracker

JT会安排其他TaskTracker重新运行失败TaskTracker的任务

### combine & Merge

#### 使用Combine

- 减少shuffle数据量
- 减少reduce过程需要处理的数据量

Combine:<a,2>

Merge:<a,<1,1>>

## HDFS

1. 大规模文件存储
2. HDFS没有锁，不能读写，写写。
3. 一次写入多次读取：避免读写冲突？

